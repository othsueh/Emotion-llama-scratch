# Emotion-llama-scratch
## Research Direction
- Multi-label approach v.s. Uni-label approach
- Impact of prompting
- W/O one or multi modality
- Unbalance dataset issue
- Make a Demo System(Interface)
## Difference between our code and [Emotion-LLaMA](https://github.com/ZebangCheng/Emotion-LLaMA?tab=readme-ov-file)
### Data Configuration
In original code, author use [`first_face.py`](https://github.com/othsueh/Emotion-llama-scratch/blob/main/minigpt4/datasets/datasets/first_face.py) and [`featureface.yaml`](https://github.com/othsueh/Emotion-llama-scratch/blob/main/minigpt4/configs/datasets/firstface/featureface.yaml) to set how to read MERR dataset. 
Here I replicate `first_face.py` with name [`iemocap.py`](https://github.com/othsueh/Emotion-llama-scratch/blob/main/minigpt4/datasets/datasets/iemocap.py) and modify the logic to fit IEMOCAP dataset.\
https://github.com/othsueh/Emotion-llama-scratch/blob/31a598808704ba9b60342c57f4c71bade524bc4a/minigpt4/datasets/datasets/iemocap.py#L34-L42
https://github.com/othsueh/Emotion-llama-scratch/blob/31a598808704ba9b60342c57f4c71bade524bc4a/minigpt4/datasets/datasets/iemocap.py#L55-L60
https://github.com/othsueh/Emotion-llama-scratch/blob/31a598808704ba9b60342c57f4c71bade524bc4a/minigpt4/datasets/datasets/iemocap.py#L129-L142
In `iemocap.py`, there are still some works to do:
- Multi-label processing: Change the logic of processing emotions, present is Uni-label only. And also need to modify prompt in this code and [generating emotion part](https://github.com/othsueh/Emotion-llama-scratch/blob/31a598808704ba9b60342c57f4c71bade524bc4a/minigpt4/models/modeling_llama.py#L88-L102) in model section. 
- W/O one or multi modality: Modify the logic of `get` method in this code to let model read empty features.
- Prompting: Add more robust prompt to `emotion_instruction_pool` to evaluate the result of the prompt.
- Narrow emotion types: I used all emotions in IEMOCAP to train the model, we can use 4-emotions and 6-emotions to compare the performance between number of emotion used.

I didn't change the configuration structure in  `featureface.yaml`. I just replace the `ann_path` with my train set and `image_path` to read image input.
https://github.com/othsueh/Emotion-llama-scratch/blob/31a598808704ba9b60342c57f4c71bade524bc4a/minigpt4/configs/datasets/firstface/featureface.yaml#L1-L6

### Train Configuration
https://github.com/othsueh/Emotion-llama-scratch/blob/31a598808704ba9b60342c57f4c71bade524bc4a/train_configs/Emotion-LLaMA_finetune.yaml#L12-L18
In this code segment I change the `ckpt` and `llama_model` to represent true model checkpoint location in my computer. And I change the `image_size` to 224 (original: 448) since our input image size of IEMOCAP is 360x240.
https://github.com/othsueh/Emotion-llama-scratch/blob/31a598808704ba9b60342c57f4c71bade524bc4a/train_configs/Emotion-LLaMA_finetune.yaml#L37
I also change the checkpoint output directory to true locatoin in my computer, it's convinent to set output location to my desired directory.

### Evaluation Configuration
The codes that concern with evaluation are `eval_emotion.yaml` and `eval_emotion.py`. \
In [`eval_emotion.yaml`](https://github.com/othsueh/Emotion-llama-scratch/blob/main/eval_configs/eval_emotion.yaml#L10), I change following properties:
- image_size: 224
- eval_file_path: Validation set or Test set location
- image_path
- ckpt
- llama_model
- save_path: Where the result should be saved. 

I made many alterations in `eval_emotion.py`, basically include the logic of reading test set, validate the result generated by model and set evaluation criteria.
https://github.com/othsueh/Emotion-llama-scratch/blob/31a598808704ba9b60342c57f4c71bade524bc4a/eval_emotion.py#L104-L109
Code part of save test result to .txt file.
https://github.com/othsueh/Emotion-llama-scratch/blob/31a598808704ba9b60342c57f4c71bade524bc4a/eval_emotion.py#L112-L120
Code part of making confusion matrix and heatmap then save the result.
https://github.com/othsueh/Emotion-llama-scratch/blob/31a598808704ba9b60342c57f4c71bade524bc4a/eval_emotion.py#L75-L80
Test set still need to go through `iemocap.py` to processing data first, setting of these entries can be checked in `iemocap.py`. 

### CrossValidate Configuration
I made `cross_train.py` to automate whole train and test process(5-cross validation + test), this takes around 13 hours to process.
https://github.com/othsueh/Emotion-llama-scratch/blob/31a598808704ba9b60342c57f4c71bade524bc4a/cross_train.py#L6-L9
https://github.com/othsueh/Emotion-llama-scratch/blob/31a598808704ba9b60342c57f4c71bade524bc4a/cross_train.py#L14-L16
Here I initialize all the configuration file path and set model checkpoint path let it can iterate within 5-folds.

Before you run this process, make sure you set right path for the files and set right output path of the result(Prevent overwriting files!).  
